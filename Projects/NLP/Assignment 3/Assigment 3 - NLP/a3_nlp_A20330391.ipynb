{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "import io\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Reading a file named file3.txt. I have attached the files as well.\n",
    "If you are using your own files please just change the name of the file read\n",
    "the join function helps combining different paragraphs\n",
    "'''\n",
    "\n",
    "filename = 'file.txt'\n",
    "text = \" \".join(io.open(filename, encoding='utf8').readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "I chose to tokeize the text myself and instead of using the nltk package.\n",
    "I have used the regular expressions to tokeize\n",
    "'''\n",
    "\n",
    "tokens = re.sub(\"\\W+\", \" \", text).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parts of speech tagging with the use of nltk package\n",
    "pos_tag= nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe format I have used is a direct approach.\\nI have gone step by step and obtained a result.\\nI have used comments and Markdowns in order to explain what I am doing.\\nAlso modules such as this in the form of comments have been used to explain.\\n'"
      ]
     },
     "execution_count": 312,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "'''\n",
    "The format I have used is a direct approach.\n",
    "I have gone step by step and obtained a result.\n",
    "I have used comments and Markdowns in order to explain what I am doing.\n",
    "Also modules such as this in the form of comments have been used to explain.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Collecting the nouns from the tagged, tokenized text \n",
    "and placing it in a list.\n",
    "'''\n",
    "noun_text = []\n",
    "noun_list = []\n",
    "for i in pos_tag:\n",
    "    if i[1] == 'NN':\n",
    "        noun_list.append(i[0]) # i[0] to collect the words, not the tags\n",
    "noun_text = list(set(noun_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting the words in the text\n",
    "counted = {}\n",
    "count = Counter()\n",
    "count.update(tokens)\n",
    "counted = dict(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This component extracts the noun synsets, i.e.,\n",
    "the synsets for every word present in the noun_list.\n",
    "''' \n",
    "word_syns = {}\n",
    "for n in noun_text:\n",
    "    synset_list = wn.synsets(n)\n",
    "    word_syns[n] = synset_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Original code to extract the hypernyms for all the synsets from above.\\nPlease remove the comments and run the original for the original result.\\nComment the modified components to run the original code ithout interruprions.\\n'"
      ]
     },
     "execution_count": 316,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "'''Original code to extract the hypernyms for all the synsets from above.\n",
    "Please remove the comments and run the original for the original result.\n",
    "Comment the modified components to run the original code ithout interruprions.\n",
    "'''\n",
    "#hyponymSet = {}\n",
    "# for i in word_syns:\n",
    "#     hypo = []\n",
    "#     if word_syns[i]:\n",
    "#         for j in word_syns[i]:\n",
    "#             var1=j.hyponyms()\n",
    "#             if var1:\n",
    "#                 hypo.append(var1[0])\n",
    "#             hyponymSet[i] = hypo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The modification/enhancement as asked:\n",
    "hyponymSet = {}\n",
    "for i in word_syns:\n",
    "    hypo = []\n",
    "    if word_syns[i]:\n",
    "        for j in word_syns[i]:\n",
    "            var1=j.hyponyms()\n",
    "            hypo.append(var1)\n",
    "            mergeList = list(itertools.chain(*hypo)) \n",
    "        hyponymSet[i] = mergeList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Original code to extract the hyponyms for all the synsets from above.\\nPlease remove the comments and run the original for the original result.\\nComment the modified components to run the original code ithout interruprions.\\n'"
      ]
     },
     "execution_count": 318,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "'''Original code to extract the hyponyms for all the synsets from above.\n",
    "Please remove the comments and run the original for the original result.\n",
    "Comment the modified components to run the original code ithout interruprions.\n",
    "'''\n",
    "# hypernymSet = {}\n",
    "# for i in word_syns:\n",
    "#     hyper = []\n",
    "#     if word_syns[i]:\n",
    "#         for j in word_syns[i]:\n",
    "#             var2=j.hypernyms()\n",
    "#             if var2:\n",
    "#                 hyper.append(var2[0])\n",
    "#                 hypernymSet[i] = hyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The modification/enhancement as asked:\n",
    "hyperSet = {}\n",
    "for i in word_syns:\n",
    "    hyper = []\n",
    "    if word_syns[i]:\n",
    "        for j in word_syns[i]:\n",
    "            var1=j.hypernyms()\n",
    "            hyper.append(var1)\n",
    "            mergeList = list(itertools.chain(*hyper)) \n",
    "        hypernymSet[i] = mergeList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enhancement meronym:\n",
    "meronymSet = {}\n",
    "for i in word_syns:\n",
    "    meronym = []\n",
    "    if word_syns[i]:\n",
    "        for j in word_syns[i]:\n",
    "            var3 = j.part_meronyms()\n",
    "            meronym.append(var3)\n",
    "            merged = list(itertools.chain(*meronym))\n",
    "        meronymSet[i] = merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Original code to merge all the components' results.\\nThe synsets, hypernyms and hyponyms are placed into a dictionary.\\nPlease remove the comments and run the original for the original result.\\nComment the modified components to run the original code ithout interruprions.\\n\""
      ]
     },
     "execution_count": 321,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "'''Original code to merge all the components' results.\n",
    "The synsets, hypernyms and hyponyms are placed into a dictionary.\n",
    "Please remove the comments and run the original for the original result.\n",
    "Comment the modified components to run the original code ithout interruprions.\n",
    "'''\n",
    "#Original merge dictionary:\n",
    "# mergeSets = {}\n",
    "# for i in word_syns:\n",
    "#     merge_list = []\n",
    "#     var4 = word_syns[i]\n",
    "#     if var4:\n",
    "#         merge_list.append(word_syns[i])\n",
    "#     if i in hyponymSet:\n",
    "#         merge_list.append(hyponymSet[i])\n",
    "#     if i in hypernymSet:\n",
    "#         merge_list.append(hypernymSet[i])\n",
    "#     merged = list(itertools.chain(*merge_list))\n",
    "#     mergeSets[i] = merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the above components, I have used dictionaries and set() functions \\nand placed the lists that contain the results in a dictionary.\\n==>This is to prevent duplicates from forming.\\n'"
      ]
     },
     "execution_count": 322,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "'''In the above components, I have used dictionaries and set() functions \n",
    "and placed the lists that contain the results in a dictionary.\n",
    "==>This is to prevent duplicates from forming.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeSets = {}\n",
    "for i in word_syns:\n",
    "    merge_list = []\n",
    "    var4 = word_syns[i] #Checking if it is an empty set or not.(Checking for availabilty)\n",
    "    if var4:\n",
    "        merge_list.append(word_syns[i])\n",
    "    if i in hyponymSet:\n",
    "        merge_list.append(hyponymSet[i])    \n",
    "    if i in hypernymSet:\n",
    "        merge_list.append(hypernymSet[i])\n",
    "    if i in meronymSet:\n",
    "        merge_list.append(meronymSet[i])\n",
    "    merged = list(itertools.chain(*merge_list))\n",
    "    mergeSets[i] = merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The lemmas conatin the words for the synsets.\n",
    "\n",
    "lemmas = {}\n",
    "for i in mergeSets:\n",
    "    words = []\n",
    "    for j in range(len(mergeSets[i])):\n",
    "        y = mergeSets[i][j].lemma_names()\n",
    "        words.append(y)\n",
    "    merged = list(itertools.chain(*words))\n",
    "    lemmas[i] = list(set(merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The trial and error method used to obtain, what i felt, the best results.\n",
    "The logic is to use dictionaries to store results and feed it to the next,\n",
    "such that a lexical chain is formed.\n",
    "'''\n",
    "\n",
    "test = {}\n",
    "for i in lemmas:\n",
    "    test1 =[]\n",
    "    for j in lemmas:\n",
    "        if i != j:\n",
    "            for x in lemmas[i]:\n",
    "                if x in lemmas[j]:\n",
    "                    test1.append(j)\n",
    "    test[i] = list(set(test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_chain = []\n",
    "for i in test:\n",
    "    x = test[i]\n",
    "    x.insert(0, i)\n",
    "    test_chain.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_text_order = []\n",
    "for i in noun_text:\n",
    "    if i not in noun_text_order:\n",
    "        noun_text_order.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_chain = {}\n",
    "for i in noun_text_order:\n",
    "    test1 =[]\n",
    "\n",
    "    for j in noun_text_order:\n",
    "        if i != j:\n",
    "            flag = False\n",
    "            count = 0\n",
    "            while(flag == False):\n",
    "                if count >= len(lemmas[i]):\n",
    "                    flag = True\n",
    "                elif (lemmas[i][count] in set(lemmas[j])) and count < len(lemmas[i]):\n",
    "                    test1.append(j)\n",
    "                    flag = True\n",
    "                else:\n",
    "                    count = count + 1\n",
    "    lexical_chain[i] = list(set(test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX2 = []\n",
    "for i in lexical_chain:\n",
    "    x = lexical_chain[i]\n",
    "    x.insert(0,i)\n",
    "    testX2.append(x)\n",
    "\n",
    "testC = testX2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX3 =[]\n",
    "for i in noun_text_order:\n",
    "    flag = False\n",
    "    for j in testX3:\n",
    "        if i in set(j):\n",
    "            flag = True\n",
    "    if flag == False:\n",
    "        xx = []\n",
    "        for j in testC:\n",
    "            if i in set(j):\n",
    "                xx.append(j)\n",
    "                testC.remove(j)\n",
    "        merged = list(itertools.chain(*xx))\n",
    "        testX3.append(list(set(merged)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "testC2 = testX3\n",
    "for i in testC2:\n",
    "    for j in i:\n",
    "        for x in testC2:\n",
    "            if i != x:\n",
    "                for m in x:\n",
    "                    if j == m:\n",
    "                        x.remove(j)\n",
    "#print test_chain_copy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain 1 :  [(u'website', 1)]\n",
      "Chain 2 :  [(u'license', 1)]\n",
      "Chain 3 :  [(u'use', 1), (u'language', 1), (u'database', 3), (u'compiler', 1), (u'number', 1), (u'grind', 1), (u'style', 1), (u'browser', 1), (u'software', 1)]\n",
      "Chain 4 :  [(u'combination', 1)]\n",
      "Chain 5 :  [(u'text', 1), (u'synonyms', 1), (u'intelligence', 1)]\n",
      "Chain 6 :  [(u'lexicographer', 1)]\n",
      "Chain 7 :  [(u'thesaurus', 1)]\n",
      "Chain 8 :  [(u'analysis', 1)]\n",
      "Chain 9 :  [(u'download', 1)]\n"
     ]
    }
   ],
   "source": [
    "best = []\n",
    "for x in testC2:\n",
    "    temp_list = []\n",
    "    for y in x:\n",
    "        temp_list.append((y,(counted[y])))\n",
    "    best.append(temp_list)\n",
    "\n",
    "for i in range(len(best)):\n",
    "    print \"Chain %d : \" %(i+1),best[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQualitative analysis:\\nWe have the results obtained before and after the modifications.\\nThe modifications and the inclusion of meronym extraction has improved the chains formed.\\nSince more than one level has been imposed there is a chain formed with words that make more sense when they are together.\\n\\nImprovement Suggestion:\\nResearch is needed for this, as we can deduce an algorithm that keeps track of the levels\\nof calculating efficiently the levels of hypernomy/hyponmy.\\nIf there is no relation whatsover in the existing words they dont have to be part of the chain.\\nThis algorithms is basically concerned with the scores that need to be assigned to the words and can be eliminate,\\nif they are not related to the topic in question.\\n'"
      ]
     },
     "execution_count": 334,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "'''\n",
    "Qualitative analysis:\n",
    "We have the results obtained before and after the modifications.\n",
    "The modifications and the inclusion of meronym extraction has improved the chains formed.\n",
    "Since more than one level has been imposed there is a chain formed with words that make more sense when they are together.\n",
    "\n",
    "Improvement Suggestion:\n",
    "Research is needed for this, as we can deduce an algorithm that keeps track of the levels\n",
    "of calculating efficiently the levels of hypernomy/hyponmy.\n",
    "If there is no relation whatsover in the existing words they dont have to be part of the chain.\n",
    "This algorithms is basically concerned with the scores that need to be assigned to the words and can be eliminate,\n",
    "if they are not related to the topic in question.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}